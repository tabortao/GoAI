# AI 文本处理服务项目需求

## 1. 项目概述

设计并实现一个多功能的AI文本处理服务。该服务需支持标准的HTTP API调用和功能丰富的命令行（CLI）两种交互模式。核心功能将通过集成LangChain框架实现，以支持多种大型语言模型（LLM），并通过Docker容器化技术实现灵活、可靠的部署。

## 2. 项目特点

- **双模式支持**：通过单一代码库同时提供RESTful API服务和交互式命令行工具，共享核心业务逻辑，确保功能一致性。
- **高度灵活性**：支持通过环境变量动态配置LLM提供商（如OpenAI、Ollama等），方便在不同模型间切换。
- **生产就绪**：内置结构化日志、统一配置管理、API健康检查和优雅关机机制，满足生产环境部署要求。
- **可扩展架构**：采用模块化和接口驱动的设计，确保代码清晰、易于维护，并方便未来扩展新功能或集成更多LLM。
- **容器化部署**：提供Dockerfile和docker-compose.yml，实现一键式构建和部署，简化开发、测试和生产流程。

## 3. 功能需求

### 3.1. 核心服务 (`internal/core`)
- **文本生成**：接收用户输入文本（Prompt），调用LLM生成回应。
- **模板化调用**：支持使用预设模板生成Prompt，简化常用场景的调用。
- **流式响应**：API和CLI均需支持流式输出，实时返回LLM生成的内容。

### 3.2. HTTP API (`internal/api`)
- **POST /api/v1/generate**：
  - **功能**：接收文本生成请求。
  - **请求体**：
    ```json
    {
      "prompt": "用户的输入文本",
      "stream": false, // 是否流式返回
      "model": "openai" // 可选，指定模型
    }
    ```
  - **响应**：
    - 非流式：返回包含完整生成结果的JSON对象。
    - 流式：返回`text/event-stream`格式的数据流。
- **GET /health**：
  - **功能**：健康检查端点，返回服务运行状态。
  - **响应**：`{"status": "ok"}`

### 3.3. 命令行工具 (`internal/cli`)
- **`goai generate "prompt"`**：
  - **功能**：执行一次性文本生成。
  - **参数**：
    - `--stream`：启用流式输出。
    - `--model <name>`：指定要使用的LLM（如`openai`或`ollama`）。
- **`goai chat`**：
  - **功能**：启动一个交互式聊天会话，持续与LLM对话。
- **`goai server`**：
  - **功能**：以服务模式启动HTTP API。

## 4. 非功能需求

- **性能**：API响应时间（不含LLM处理时间）应低于100ms。
- **可靠性**：服务应支持优雅关机，在接收到终止信号时能正确处理完当前请求。
- **安全性**：API密钥等敏感信息必须通过环境变量加载，禁止硬编码在代码中。
- **可维护性**：代码遵循Go语言最佳实践，模块边界清晰，并提供必要的单元测试和集成测试。

## 5. 技术栈与开发要求

- **编程语言**：Go 1.24.1
- **核心框架**：
  - **HTTP服务**：Gin
  - **命令行**：Cobra
  - **LLM集成**：LangChainGo
- **配置管理**：Viper (用于读取`.env`文件和环境变量)
- **日志**：slog (Go原生结构化日志库)
- **部署**：Docker, Docker Compose

## 6. 项目结构

```
GoAI/
├── cmd/
│   ├── server/
│   │   └── main.go        # HTTP服务入口，负责启动Gin服务
│   └── cli/
│       └── main.go        # 命令行应用入口，负责初始化Cobra
├── internal/
│   ├── api/
│   │   ├── handler.go     # API请求处理器，调用核心服务
│   │   └── router.go      # API路由定义与中间件配置
│   ├── cli/
│   │   └── commands.go    # Cobra命令定义（generate, chat, server）
│   ├── config/
│   │   └── config.go      # 配置加载与管理（环境变量、.env文件）
│   ├── core/
│   │   └── service.go     # 核心业务逻辑，封装LangChain调用
│   ├── llm/
│   │   ├── manager.go     # LLM实例管理器，根据配置创建和获取LLM客户端
│   │   ├── openai.go      # OpenAI LLM实现
│   │   └── ollama.go      # Ollama LLM实现
│   └── models/
│       └── request.go     # API请求和响应的数据结构定义
├── pkg/
│   └── utils/
│       ├── logger.go      # 日志组件初始化
│       └── template.go    # 文本模板处理工具
├── .env.example           # 环境变量示例文件（API Keys, 配置等）
├── Dockerfile             # 用于构建生产镜像
├── docker-compose.yml     # 用于本地开发和测试环境编排
├── go.mod
├── go.sum
└── README.md              # 项目说明、构建和运行指南
```

## 7. 部署方案

- **镜像构建**：`Dockerfile`将构建一个包含Go编译环境的多阶段镜像，最终生成一个轻量化的可执行文件。
- **入口点**：通过一个`entrypoint.sh`脚本根据启动参数（如`server`或`generate`）决定是运行API服务还是执行命令行命令。
- **本地开发**：使用`docker-compose.yml`启动服务及其依赖（如Ollama），并支持热重载。
- **生产部署**：可直接在支���Docker的任何环境中运行容器。

## 8. 开发与测试

- **单元测试**：针对核心服务、LLM管理器和工具函数编写单元测试。
- **集成测试**：为API端点和CLI命令编写集成测试，验证端到端功能。
- **CI/CD**：建议配置GitHub Actions或类似工具，在代码提交时自动运行测试、代码检查和镜像构建。